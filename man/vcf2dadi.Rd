% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/vcf2dadi.R
\name{vcf2dadi}
\alias{vcf2dadi}
\title{Create a \code{dadi} SNP input file from a any vcf file.}
\arguments{
\item{fasta.outgroup}{(optional) The fasta output file from STACKS. This file is 
required to use an outgroup. Default: \code{fasta.outgroup = NULL}.}

\item{fasta.ingroup}{(optional) The fasta output file from STACKS. This file is 
required to use with an outgroup. Leave empty if no outgroup. 
Default: \code{fasta.ingroup = NULL}.}

\item{sumstats.outgroup}{(optional) The sumstats output file from STACKS 
when running STACKS for the outgroup fasta file. This file is 
required to use an outgroup. Default: \code{sumstats.outgroup = NULL}.}

\item{sumstats.ingroup}{(optional) The sumstats output file from STACKS
when running STACKS for the ingroup fasta file.This file is 
required to use with an outgroup. Leave empty if no outgroup. 
Default: \code{sumstats.ingroup = NULL}.}

\item{dadi.input.filename}{(optional) Name of the \code{dadi} SNP input file 
written to the working directory. e.g. \code{dadi.file.tsv}. 
Default use date and time to make the file. If used, the file extension
need to finish with \code{.tsv or .txt}.}
}
\value{
The function returns a list with 1 or 2 objects (w/o imputations): 
`$dadi.no.imputation`
`$dadi.imputed`
The data frame are accessed form the list with `$`.
}
\description{
This function will create a \code{dadi} SNP input file using a
VCF file (Danecek et al. 2011). Missing data can bias demographic inference, 
`vcf2dadi` was created to address this problem, providing a customizable 
imputation framework specifically designed to work with GBS/RAD data.
If your VCF is not filtered, you can supply the function a whitelist of loci and a 
blacklist of individuals.
}
\details{
\strong{Input files:}
\enumerate{
\item VCF file (e.g. \code{data = "batch_1.vcf"}). 
To make the VCF population ready, you need the \code{strata} argument.

\item haplotype file created in STACKS (e.g. \code{data = "batch_1.haplotypes.tsv"}).
To make the haplotype file population ready, you need the \code{strata} argument.

\item Data frame
Tab delimitted.
\strong{2 genotypes formats are available, both use 3 character per allele:}
6 characters no allele separator: e.g. \code{001002 of 111333} (for heterozygote individual).
6 characters WITH an allele separator: e.g. \code{001/002 of 111/333} (for heterozygote individual).
The separator can be any of these: \code{"/", ":", "_", "-", "."}.
Missing alleles are coded \code{000}.
To discriminate the long from the wide format, 
the function \pkg{stackr} \code{\link[stackr]{read_long_tidy_wide}} searches 
for columns number, > 20 for wide 
(i.e. don't use less than 10 markers in wide format, the function was not designed for that).

Data Frame wide format:
The wide format cannot store metadata info.
The wide format contains starts with these 2 id columns: 
\code{INDIVIDUALS}, \code{POP_ID} (that refers to any grouping of individuals), 
the remaining columns are the markers in separate columns storing genotypes.
This format requires column numbers to be larger than 20.
Data frame long/tidy format:
This format requires column numbers to be within the range: 4 min - 20 max.
The long format is considered to be a tidy data frame and can store metadata info. 
(e.g. from a VCF see \pkg{stackr} \code{\link[stackr]{tidy_genomic_data}}). The 4 columns
required in the long format are: \code{INDIVIDUALS}, \code{POP_ID}, 
\code{MARKERS} and \code{GENOTYPE or GT}.

Note that the \code{POP_ID} column can be any hierarchical grouping. 
See the argument \code{strata} for other means of controlling grouping used 
in the assignment.

\item PLINK file in 
\code{tped/tfam} format (e.g. \code{data =  "data.assignment.tped"}). 
The first 2 columns of the \code{tfam} file will be used for the 
\code{strata} argument below, unless a new one is provided. 
Columns 1, 3 and 4 of the \code{tped} are discarded. The remaining columns 
correspond to the genotype in the format \code{01/04} 
where \code{A = 01, C = 02, G = 03 and T = 04}. For \code{A/T} format, use 
PLINK or bash to convert.
Use \href{http://vcftools.sourceforge.net/}{VCFTOOLS} with \code{--plink-tped} 
to convert very large VCF file. For \code{.ped} file conversion to 
\code{.tped} use \href{http://pngu.mgh.harvard.edu/~purcell/plink/}{PLINK} 
with \code{--recode transpose},

\item \code{\link[adegenet]{genind}} object from \code{\link[adegenet]{adegenet}}.

\item genepop data file (e.g. \code{data = kiwi_data.gen}). Here, the function can only use
alleles encoded with 3 digits.
}


\strong{Imputations details:}
The imputations using Random Forest requires more time to compute and can take several
minutes and hours depending on the size of the dataset and polymorphism of
the species used. e.g. with a low polymorphic taxa, and a data set 
containing 30\% missing data, 5 000 haplotypes loci and 500 individuals 
will require 15 min.
}
\examples{
#See vignette `vignette_vcf2dadi` for more info.
\dontrun{
dadi.no.imputation <- vcf2dadi(
data = "batch_1.vcf", 
whitelist.markers = "whitelist.loci.txt",
strata = "strata.file.tsv",
pop.levels = c("PAN", "COS"),
common.markers = TRUE, 
fasta.ingroup = "batch_1.ingroup.fa", 
fasta.outgroup = "batch_1.outgroup.fa", 
sumstats.ingroup = "batch_1.sumstats.ingroup.tsv", 
sumstats.outgroup = "batch_1.sumstats.outgroup.tsv"
)

With Imputations:
dadi.files <- vcf2dadi(
data = "batch_1.vcf", 
whitelist.markers = "whitelist.loci.txt",
strata = "strata.file.tsv",
pop.levels = c("PAN", "COS"),
common.markers = TRUE, 
fasta.ingroup = "batch_1.ingroup.fa", 
fasta.outgroup = "batch_1.outgroup.fa", 
sumstats.ingroup = "batch_1.sumstats.ingroup.tsv", 
sumstats.outgroup = "batch_1.sumstats.outgroup.tsv",
imputation.method = "max", 
impute = "allele", 
imputations.group = "populations", 
num.tree = 100, 
iteration.rf = 10, 
split.number = 100, 
verbose = FALSE, 
parallel.core = 8
)
# to get the imputed data frame:
dadi.imputed.df <- dadi.files$dadi.imputed
}
}
\author{
Thierry Gosselin \email{thierrygosselin@icloud.com} and
Anne-Laure Ferchaud \email{annelaureferchaud@gmail.com}
}
\references{
Catchen JM, Amores A, Hohenlohe PA et al. (2011) 
Stacks: Building and Genotyping Loci De Novo From Short-Read Sequences. 
G3, 1, 171-182.

Catchen JM, Hohenlohe PA, Bassham S, Amores A, Cresko WA (2013) 
Stacks: an analysis tool set for population genomics. 
Molecular Ecology, 22, 3124-3140.

Danecek P, Auton A, Abecasis G et al. (2011)
The variant call format and VCFtools.
Bioinformatics, 27, 2156-2158.

Gutenkunst RN, Hernandez RD, Williamson SH, Bustamante CD (2009)
Inferring the Joint Demographic History of Multiple Populations 
from Multidimensional SNP Frequency Data (G McVean, Ed,). 
PLoS genetics, 5, e1000695.

Ishwaran H. and Kogalur U.B. (2015). Random Forests for Survival,
 Regression and Classification (RF-SRC), R package version 1.6.1.

Ishwaran H. and Kogalur U.B. (2007). Random survival forests
for R. R News 7(2), 25-31.

Ishwaran H., Kogalur U.B., Blackstone E.H. and Lauer M.S. (2008).
Random survival forests. Ann. Appl. Statist. 2(3), 841-860.
}

